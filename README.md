# ML_GD_LinearRegression
## 以Gradient Descent(梯度下降)訓練出線性回歸

> input data ，並用散佈圖呈現  

> 將數據標準化(使數據易於處理)，並初始化具有隨機值的參數 

> 預測函數與目標函數，學習率、誤差分數與誤差參數更新達到0.01或更小

> 更新參數並計算先前錯誤的差值，並繪圖出訓練的線性回歸

## 結論
* 藉由重複調整參數更新，誤差下降到從(第一次的76273.8360 到第394次的0.0097)，並找到最佳預測函數=最佳參數（θ）。
* 訓練模型代表找到一組模型參數，這組參數可以在訓練集上使的損失函數最小，而使用梯度下降需要計算每個θ下損失函數的梯度，學習率與梯度向量決定了每一步的大 
  小。
